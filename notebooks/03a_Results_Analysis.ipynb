{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üéØ SmartMatch Resume Analyzer - Part 3a: Results Analysis\n",
    "\n",
    "> **Live AI analysis demo and comprehensive results interpretation**\n",
    "\n",
    "This is the first part of our results analysis notebook. Here we'll run live AI analysis and examine the core results.\n",
    "\n",
    "## üìö Tutorial Series\n",
    "\n",
    "1. **Part 1: Setup and Data** - Environment setup, dependencies, and data models\n",
    "2. **Part 2: Analysis Pipeline** - Core AI analysis engine and LangChain integration  \n",
    "3. **Part 3a: Results Analysis** (This notebook) - Running analyses and core results\n",
    "4. **Part 3b: Interpretation Insights** - Technical deep dive and production patterns\n",
    "\n",
    "## üìã What You'll Learn\n",
    "\n",
    "- **Live AI Analysis**: Run the complete analysis pipeline\n",
    "- **Results Interpretation**: Understand match scores, keywords, and suggestions\n",
    "- **Performance Metrics**: Analyze processing speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìã Prerequisites\n",
    "\n",
    "Complete setup from previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete setup from Parts 1 & 2\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Pydantic for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Enable async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üìù Load Sample Data\n",
    "\n",
    "Load our sample resume and job description for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample resume - Software Engineer transitioning to ML (from Part 1)\n",
    "SAMPLE_RESUME = \"\"\"\n",
    "John Smith\n",
    "Software Engineer\n",
    "Email: john.smith@email.com\n",
    "\n",
    "PROFESSIONAL SUMMARY\n",
    "Experienced software engineer with 5+ years developing scalable web applications and data pipelines.\n",
    "Strong background in Python, cloud technologies, and agile development practices.\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "Languages: Python, JavaScript, SQL, Java\n",
    "Frameworks: Django, Flask, React, Node.js\n",
    "Databases: PostgreSQL, MongoDB, Redis\n",
    "Cloud: AWS (EC2, S3, Lambda), Docker, Kubernetes\n",
    "Tools: Git, Jenkins, JIRA, Prometheus\n",
    "\n",
    "EXPERIENCE\n",
    "Senior Software Engineer | TechCorp | 2021-2024\n",
    "‚Ä¢ Developed real-time data processing pipeline using Apache Kafka handling 100k+ messages/hour\n",
    "‚Ä¢ Optimized database queries improving response time by 40% through indexing and query optimization\n",
    "‚Ä¢ Led team of 3 engineers in implementing microservices architecture using Docker and Kubernetes\n",
    "‚Ä¢ Mentored junior developers and conducted code reviews maintaining 95% code quality standards\n",
    "\n",
    "Software Engineer | StartupXYZ | 2019-2021\n",
    "‚Ä¢ Built REST APIs using Django and Flask serving 10,000+ daily active users\n",
    "‚Ä¢ Implemented automated testing and CI/CD pipelines reducing deployment time by 60%\n",
    "‚Ä¢ Collaborated with product team using agile methodologies and sprint planning\n",
    "\n",
    "EDUCATION\n",
    "Bachelor of Science in Computer Science | University of Technology | 2019\n",
    "\"\"\"\n",
    "\n",
    "# Sample job description - Machine Learning Engineer\n",
    "SAMPLE_JOB_DESCRIPTION = \"\"\"\n",
    "Machine Learning Engineer\n",
    "Company: AI Innovations Inc.\n",
    "\n",
    "We are seeking a skilled Machine Learning Engineer to join our AI team and help build next-generation ML solutions.\n",
    "\n",
    "REQUIREMENTS:\n",
    "‚Ä¢ 3+ years of experience in machine learning and data science\n",
    "‚Ä¢ Strong proficiency in Python and machine learning frameworks (TensorFlow, PyTorch, Scikit-learn)\n",
    "‚Ä¢ Experience with MLOps practices, model deployment, and monitoring\n",
    "‚Ä¢ Knowledge of deep learning, neural networks, and NLP techniques\n",
    "‚Ä¢ Experience with cloud platforms (AWS, GCP) and containerization (Docker)\n",
    "‚Ä¢ Strong background in statistics, mathematics, and data analysis\n",
    "‚Ä¢ Experience with model training, evaluation, and optimization\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "‚Ä¢ Design and implement machine learning models for various business problems\n",
    "‚Ä¢ Build and maintain ML pipelines from data ingestion to model deployment\n",
    "‚Ä¢ Collaborate with data scientists and engineers to productionize ML solutions\n",
    "‚Ä¢ Monitor model performance and implement improvements\n",
    "‚Ä¢ Research and evaluate new ML techniques and technologies\n",
    "\n",
    "PREFERRED QUALIFICATIONS:\n",
    "‚Ä¢ MS/PhD in Computer Science, Machine Learning, or related field\n",
    "‚Ä¢ Experience with distributed computing and big data technologies\n",
    "‚Ä¢ Publications in ML conferences or journals\n",
    "‚Ä¢ Experience with recommendation systems, computer vision, or NLP\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Sample data loaded:\")\n",
    "print(f\"   Resume: {len(SAMPLE_RESUME)} characters\")\n",
    "print(f\"   Job Description: {len(SAMPLE_JOB_DESCRIPTION)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ü§ñ Initialize AI Analyzer\n",
    "\n",
    "Load the complete ResumeAnalyzer from Part 2 (simplified for demo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data models and simplified analyzer\n",
    "class BulletSuggestion(BaseModel):\n",
    "    original: str = Field(..., description=\"Original bullet point\")\n",
    "    improved: str = Field(..., description=\"AI-improved version\")\n",
    "    reason: str = Field(..., description=\"Explanation of improvements\")\n",
    "\n",
    "class AnalysisResponse(BaseModel):\n",
    "    match_percentage: float = Field(..., ge=0, le=100, description=\"Match percentage\")\n",
    "    matched_keywords: List[str] = Field(default=[], description=\"Keywords found in both texts\")\n",
    "    missing_keywords: List[str] = Field(default=[], description=\"Job keywords missing from resume\")\n",
    "    suggestions: List[BulletSuggestion] = Field(default=[], description=\"Improvement suggestions\")\n",
    "    strengths: List[str] = Field(default=[], description=\"Resume strengths\")\n",
    "    areas_for_improvement: List[str] = Field(default=[], description=\"Areas needing improvement\")\n",
    "    overall_feedback: str = Field(..., description=\"Summary feedback\")\n",
    "    processing_time: Optional[float] = Field(None, description=\"Analysis processing time\")\n",
    "\n",
    "# Simplified analyzer for demo\n",
    "class ResumeAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "    \n",
    "    async def analyze(self, resume_text: str, job_description: str) -> AnalysisResponse:\n",
    "        \"\"\"Simplified analysis for demo.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Extract keywords (simplified)\n",
    "        resume_keywords = self._extract_keywords_simple(resume_text)\n",
    "        job_keywords = self._extract_keywords_simple(job_description)\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        match_result = self._simple_keyword_match(resume_keywords, job_keywords)\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return AnalysisResponse(\n",
    "            match_percentage=match_result[\"match_percentage\"],\n",
    "            matched_keywords=match_result[\"matched_keywords\"],\n",
    "            missing_keywords=match_result[\"missing_keywords\"],\n",
    "            suggestions=[],  # Simplified for demo\n",
    "            strengths=match_result[\"strengths\"],\n",
    "            areas_for_improvement=match_result[\"improvements\"],\n",
    "            overall_feedback=f\"Your resume shows a {match_result['match_percentage']}% match with the job description.\",\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "    \n",
    "    def _extract_keywords_simple(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple keyword extraction for demo.\"\"\"\n",
    "        # Common tech keywords for demo\n",
    "        keywords = []\n",
    "        common_keywords = [\n",
    "            \"Python\", \"JavaScript\", \"SQL\", \"Java\", \"Django\", \"Flask\", \"React\", \n",
    "            \"Node.js\", \"PostgreSQL\", \"MongoDB\", \"Redis\", \"AWS\", \"Docker\", \n",
    "            \"Kubernetes\", \"Git\", \"Jenkins\", \"Machine Learning\", \"TensorFlow\", \n",
    "            \"PyTorch\", \"Scikit-learn\", \"MLOps\", \"Deep Learning\", \"Neural Networks\", \n",
    "            \"NLP\", \"Data Science\", \"Statistics\", \"Mathematics\", \"Data Analysis\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for keyword in common_keywords:\n",
    "            if keyword.lower() in text_lower:\n",
    "                keywords.append(keyword)\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def _simple_keyword_match(self, resume_keywords: List[str], job_keywords: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Simple keyword matching.\"\"\"\n",
    "        resume_set = set(k.lower() for k in resume_keywords)\n",
    "        job_set = set(k.lower() for k in job_keywords)\n",
    "        \n",
    "        matched = list(resume_set & job_set)\n",
    "        missing = list(job_set - resume_set)\n",
    "        \n",
    "        match_percentage = int((len(matched) / len(job_set)) * 100) if job_set else 0\n",
    "        \n",
    "        return {\n",
    "            \"match_percentage\": match_percentage,\n",
    "            \"matched_keywords\": [k for k in resume_keywords if k.lower() in matched],\n",
    "            \"missing_keywords\": [k for k in job_keywords if k.lower() in missing],\n",
    "            \"strengths\": [f\"Strong technical background with {len(matched)} matching skills\"],\n",
    "            \"improvements\": [f\"Consider adding: {', '.join(missing[:5])}\"] if missing else []\n",
    "        }\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = ResumeAnalyzer(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"ü§ñ SmartMatch Resume Analyzer initialized\")\n",
    "print(\"üéØ Ready for AI analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üöÄ Live AI Analysis Demo\n",
    "\n",
    "Let's run the complete AI analysis pipeline and see SmartMatch in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Starting AI analysis pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the complete analysis\n",
    "analysis_result = await analyzer.analyze(SAMPLE_RESUME, SAMPLE_JOB_DESCRIPTION)\n",
    "\n",
    "print(\"‚úÖ Analysis completed!\")\n",
    "print(f\"‚è±Ô∏è Processing time: {analysis_result.processing_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## üìä Analysis Results\n",
    "\n",
    "Let's examine the detailed results from our AI analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overall match score\n",
    "print(\"üéØ OVERALL MATCH ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"üìà Match Score: {analysis_result.match_percentage:.1f}%\")\n",
    "print(f\"üí¨ Feedback: {analysis_result.overall_feedback}\")\n",
    "print(f\"‚è±Ô∏è Processing Time: {analysis_result.processing_time:.2f}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display matched keywords\n",
    "print(\"‚úÖ MATCHED KEYWORDS\")\n",
    "print(\"=\"*30)\n",
    "for i, keyword in enumerate(analysis_result.matched_keywords, 1):\n",
    "    print(f\"{i:2d}. {keyword}\")\n",
    "print(f\"\\nTotal matches: {len(analysis_result.matched_keywords)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing keywords\n",
    "print(\"‚ùå MISSING KEYWORDS (Improvement Opportunities)\")\n",
    "print(\"=\"*50)\n",
    "for i, keyword in enumerate(analysis_result.missing_keywords, 1):\n",
    "    print(f\"{i:2d}. {keyword}\")\n",
    "print(f\"\\nTotal missing: {len(analysis_result.missing_keywords)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display strengths and improvements\n",
    "print(\"üí™ RESUME STRENGTHS\")\n",
    "print(\"=\"*25)\n",
    "for i, strength in enumerate(analysis_result.strengths, 1):\n",
    "    print(f\"{i}. {strength}\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ AREAS FOR IMPROVEMENT\")\n",
    "print(\"=\"*30)\n",
    "for i, improvement in enumerate(analysis_result.areas_for_improvement, 1):\n",
    "    print(f\"{i}. {improvement}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Continue to Part 3b\n",
    "\n",
    "Continue with **Part 3b: Interpretation Insights** to explore:\n",
    "- Technical deep dive into the analysis\n",
    "- Production patterns and best practices\n",
    "- Performance insights and takeaways\n",
    "- Next steps for extending this foundation\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the SmartMatch Resume Analyzer tutorial series - Built with ‚ù§Ô∏è using LangChain, OpenAI, and modern Python.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}