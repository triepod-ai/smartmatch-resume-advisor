{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 🎯 SmartMatch Resume Analyzer - Part 3a: Results Analysis\n",
    "\n",
    "> **Live AI analysis demo and comprehensive results interpretation**\n",
    "\n",
    "This is the first part of our results analysis notebook. Here we'll run live AI analysis and examine the core results.\n",
    "\n",
    "## 📚 Tutorial Series\n",
    "\n",
    "1. **Part 1: Setup and Data** - Environment setup, dependencies, and data models\n",
    "2. **Part 2: Analysis Pipeline** - Core AI analysis engine and LangChain integration  \n",
    "3. **Part 3a: Results Analysis** (This notebook) - Running analyses and core results\n",
    "4. **Part 3b: Interpretation Insights** - Technical deep dive and production patterns\n",
    "\n",
    "## 📋 What You'll Learn\n",
    "\n",
    "- **Live AI Analysis**: Run the complete analysis pipeline\n",
    "- **Results Interpretation**: Understand match scores, keywords, and suggestions\n",
    "- **Performance Metrics**: Analyze processing speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 📋 Prerequisites\n",
    "\n",
    "Complete setup from previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete setup from Parts 1 & 2\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Pydantic for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Enable async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 📝 Load Sample Data\n",
    "\n",
    "Load our sample resume and job description for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample resume - Software Engineer transitioning to ML (from Part 1)\n",
    "SAMPLE_RESUME = \"\"\"\n",
    "John Smith\n",
    "Software Engineer\n",
    "Email: john.smith@email.com\n",
    "\n",
    "PROFESSIONAL SUMMARY\n",
    "Experienced software engineer with 5+ years developing scalable web applications and data pipelines.\n",
    "Strong background in Python, cloud technologies, and agile development practices.\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "Languages: Python, JavaScript, SQL, Java\n",
    "Frameworks: Django, Flask, React, Node.js\n",
    "Databases: PostgreSQL, MongoDB, Redis\n",
    "Cloud: AWS (EC2, S3, Lambda), Docker, Kubernetes\n",
    "Tools: Git, Jenkins, JIRA, Prometheus\n",
    "\n",
    "EXPERIENCE\n",
    "Senior Software Engineer | TechCorp | 2021-2024\n",
    "• Developed real-time data processing pipeline using Apache Kafka handling 100k+ messages/hour\n",
    "• Optimized database queries improving response time by 40% through indexing and query optimization\n",
    "• Led team of 3 engineers in implementing microservices architecture using Docker and Kubernetes\n",
    "• Mentored junior developers and conducted code reviews maintaining 95% code quality standards\n",
    "\n",
    "Software Engineer | StartupXYZ | 2019-2021\n",
    "• Built REST APIs using Django and Flask serving 10,000+ daily active users\n",
    "• Implemented automated testing and CI/CD pipelines reducing deployment time by 60%\n",
    "• Collaborated with product team using agile methodologies and sprint planning\n",
    "\n",
    "EDUCATION\n",
    "Bachelor of Science in Computer Science | University of Technology | 2019\n",
    "\"\"\"\n",
    "\n",
    "# Sample job description - Machine Learning Engineer\n",
    "SAMPLE_JOB_DESCRIPTION = \"\"\"\n",
    "Machine Learning Engineer\n",
    "Company: AI Innovations Inc.\n",
    "\n",
    "We are seeking a skilled Machine Learning Engineer to join our AI team and help build next-generation ML solutions.\n",
    "\n",
    "REQUIREMENTS:\n",
    "• 3+ years of experience in machine learning and data science\n",
    "• Strong proficiency in Python and machine learning frameworks (TensorFlow, PyTorch, Scikit-learn)\n",
    "• Experience with MLOps practices, model deployment, and monitoring\n",
    "• Knowledge of deep learning, neural networks, and NLP techniques\n",
    "• Experience with cloud platforms (AWS, GCP) and containerization (Docker)\n",
    "• Strong background in statistics, mathematics, and data analysis\n",
    "• Experience with model training, evaluation, and optimization\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "• Design and implement machine learning models for various business problems\n",
    "• Build and maintain ML pipelines from data ingestion to model deployment\n",
    "• Collaborate with data scientists and engineers to productionize ML solutions\n",
    "• Monitor model performance and implement improvements\n",
    "• Research and evaluate new ML techniques and technologies\n",
    "\n",
    "PREFERRED QUALIFICATIONS:\n",
    "• MS/PhD in Computer Science, Machine Learning, or related field\n",
    "• Experience with distributed computing and big data technologies\n",
    "• Publications in ML conferences or journals\n",
    "• Experience with recommendation systems, computer vision, or NLP\n",
    "\"\"\"\n",
    "\n",
    "print(\"📄 Sample data loaded:\")\n",
    "print(f\"   Resume: {len(SAMPLE_RESUME)} characters\")\n",
    "print(f\"   Job Description: {len(SAMPLE_JOB_DESCRIPTION)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 🤖 Initialize AI Analyzer\n",
    "\n",
    "Load the complete ResumeAnalyzer from Part 2 (simplified for demo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data models and simplified analyzer\n",
    "class BulletSuggestion(BaseModel):\n",
    "    original: str = Field(..., description=\"Original bullet point\")\n",
    "    improved: str = Field(..., description=\"AI-improved version\")\n",
    "    reason: str = Field(..., description=\"Explanation of improvements\")\n",
    "\n",
    "class AnalysisResponse(BaseModel):\n",
    "    match_percentage: float = Field(..., ge=0, le=100, description=\"Match percentage\")\n",
    "    matched_keywords: List[str] = Field(default=[], description=\"Keywords found in both texts\")\n",
    "    missing_keywords: List[str] = Field(default=[], description=\"Job keywords missing from resume\")\n",
    "    suggestions: List[BulletSuggestion] = Field(default=[], description=\"Improvement suggestions\")\n",
    "    strengths: List[str] = Field(default=[], description=\"Resume strengths\")\n",
    "    areas_for_improvement: List[str] = Field(default=[], description=\"Areas needing improvement\")\n",
    "    overall_feedback: str = Field(..., description=\"Summary feedback\")\n",
    "    processing_time: Optional[float] = Field(None, description=\"Analysis processing time\")\n",
    "\n",
    "# Simplified analyzer for demo\n",
    "class ResumeAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "    \n",
    "    async def analyze(self, resume_text: str, job_description: str) -> AnalysisResponse:\n",
    "        \"\"\"Simplified analysis for demo.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Extract keywords (simplified)\n",
    "        resume_keywords = self._extract_keywords_simple(resume_text)\n",
    "        job_keywords = self._extract_keywords_simple(job_description)\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        match_result = self._simple_keyword_match(resume_keywords, job_keywords)\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return AnalysisResponse(\n",
    "            match_percentage=match_result[\"match_percentage\"],\n",
    "            matched_keywords=match_result[\"matched_keywords\"],\n",
    "            missing_keywords=match_result[\"missing_keywords\"],\n",
    "            suggestions=[],  # Simplified for demo\n",
    "            strengths=match_result[\"strengths\"],\n",
    "            areas_for_improvement=match_result[\"improvements\"],\n",
    "            overall_feedback=f\"Your resume shows a {match_result['match_percentage']}% match with the job description.\",\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "    \n",
    "    def _extract_keywords_simple(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple keyword extraction for demo.\"\"\"\n",
    "        # Common tech keywords for demo\n",
    "        keywords = []\n",
    "        common_keywords = [\n",
    "            \"Python\", \"JavaScript\", \"SQL\", \"Java\", \"Django\", \"Flask\", \"React\", \n",
    "            \"Node.js\", \"PostgreSQL\", \"MongoDB\", \"Redis\", \"AWS\", \"Docker\", \n",
    "            \"Kubernetes\", \"Git\", \"Jenkins\", \"Machine Learning\", \"TensorFlow\", \n",
    "            \"PyTorch\", \"Scikit-learn\", \"MLOps\", \"Deep Learning\", \"Neural Networks\", \n",
    "            \"NLP\", \"Data Science\", \"Statistics\", \"Mathematics\", \"Data Analysis\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for keyword in common_keywords:\n",
    "            if keyword.lower() in text_lower:\n",
    "                keywords.append(keyword)\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def _simple_keyword_match(self, resume_keywords: List[str], job_keywords: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Simple keyword matching.\"\"\"\n",
    "        resume_set = set(k.lower() for k in resume_keywords)\n",
    "        job_set = set(k.lower() for k in job_keywords)\n",
    "        \n",
    "        matched = list(resume_set & job_set)\n",
    "        missing = list(job_set - resume_set)\n",
    "        \n",
    "        match_percentage = int((len(matched) / len(job_set)) * 100) if job_set else 0\n",
    "        \n",
    "        return {\n",
    "            \"match_percentage\": match_percentage,\n",
    "            \"matched_keywords\": [k for k in resume_keywords if k.lower() in matched],\n",
    "            \"missing_keywords\": [k for k in job_keywords if k.lower() in missing],\n",
    "            \"strengths\": [f\"Strong technical background with {len(matched)} matching skills\"],\n",
    "            \"improvements\": [f\"Consider adding: {', '.join(missing[:5])}\"] if missing else []\n",
    "        }\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = ResumeAnalyzer(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"🤖 SmartMatch Resume Analyzer initialized\")\n",
    "print(\"🎯 Ready for AI analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 🚀 Live AI Analysis Demo\n",
    "\n",
    "Let's run the complete AI analysis pipeline and see SmartMatch in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤖 Starting AI analysis pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the complete analysis\n",
    "analysis_result = await analyzer.analyze(SAMPLE_RESUME, SAMPLE_JOB_DESCRIPTION)\n",
    "\n",
    "print(\"✅ Analysis completed!\")\n",
    "print(f\"⏱️ Processing time: {analysis_result.processing_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 📊 Analysis Results\n",
    "\n",
    "Let's examine the detailed results from our AI analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overall match score\n",
    "print(\"🎯 OVERALL MATCH ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"📈 Match Score: {analysis_result.match_percentage:.1f}%\")\n",
    "print(f\"💬 Feedback: {analysis_result.overall_feedback}\")\n",
    "print(f\"⏱️ Processing Time: {analysis_result.processing_time:.2f}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display matched keywords\n",
    "print(\"✅ MATCHED KEYWORDS\")\n",
    "print(\"=\"*30)\n",
    "for i, keyword in enumerate(analysis_result.matched_keywords, 1):\n",
    "    print(f\"{i:2d}. {keyword}\")\n",
    "print(f\"\\nTotal matches: {len(analysis_result.matched_keywords)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing keywords\n",
    "print(\"❌ MISSING KEYWORDS (Improvement Opportunities)\")\n",
    "print(\"=\"*50)\n",
    "for i, keyword in enumerate(analysis_result.missing_keywords, 1):\n",
    "    print(f\"{i:2d}. {keyword}\")\n",
    "print(f\"\\nTotal missing: {len(analysis_result.missing_keywords)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display strengths and improvements\n",
    "print(\"💪 RESUME STRENGTHS\")\n",
    "print(\"=\"*25)\n",
    "for i, strength in enumerate(analysis_result.strengths, 1):\n",
    "    print(f\"{i}. {strength}\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 AREAS FOR IMPROVEMENT\")\n",
    "print(\"=\"*30)\n",
    "for i, improvement in enumerate(analysis_result.areas_for_improvement, 1):\n",
    "    print(f\"{i}. {improvement}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Continue to Part 3b\n",
    "\n",
    "Continue with **Part 3b: Interpretation Insights** to explore:\n",
    "- Technical deep dive into the analysis\n",
    "- Production patterns and best practices\n",
    "- Performance insights and takeaways\n",
    "- Next steps for extending this foundation\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the SmartMatch Resume Analyzer tutorial series - Built with ❤️ using LangChain, OpenAI, and modern Python.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}